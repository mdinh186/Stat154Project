---
title: "Final Report, Classification with the Census Income Data Set"
author: "My Dinh"
date: "12/8/2017"
output: pdf_document
---


### I. Introduction: 

This report focuses on the classification process for adult census data from 1994. We are looking to predict whether or not a person's salary is greater or less than 50K, based off of other attributes. The purpose of this project is to construct and analyze various models generated by classification trees.

For this report, we examine and clean the various features of our original data, and then selected the features that appeared to have the most influence on income prediction. We addressed the presence of class imbalance through different sampling techniques, and also tuned parameters with cross-validation before finalizing our models.

After performing feature engineering, we model three different classification trees and analyze the results. We find that our most successful tree that we built was our final RandomForest model, which ultimately had an accuracy of ###########.

### II. Data: 

The data used for this project came from the UCI repository, donated by Ronny Kohavi and Barry Becker.  The basis of this data originated from the 1994 Census database. Multivariate data was collected onadult individuals, and was divided into a training set of 32561 observations and a test set of 16281 observations. There are 15 variables total, 6 of which are numerical and the other 9 variables are categorical. 

Although most of our integer attributes were continuous, we noticed that education-num appeared to operate like a categorical variable, and had 16 levels. Our categorical attributes, with their relative number of levels, are: workclass(6), education(16), marital-status(7), occupation (14), relationship (6), race(5), sex(2), and native-country(41). 

Income was also factored into two levels: <= 50K, and > 50K. 23.93% of the data had income >50K. After observations with missing values were removed, this probability became 24.78%. 

At first glance, the original dataset was immense; creating an interpretable classification tree from this would need thoughtful data preparation. As such, we focused heavily on properly analyzing the data and examining each variable.

### III. Data Processing/Feature Engineering: 

#### Data Cleaning


There is a total of 4262 missing values, for 2399 observations. Missing values are seen in the original dataset as '?' symbols. All the missing values belonged to three columns: occupation, nativecountry, and workclass, and multiple missing values were frequent for the same observation. As thus, it is very likely that the missing values are not at random (MNAR). 

Given the MNAR, we examined whether or not a dataset with the missing values removed had better results than a dataset with imputed values. After  

We will try to impute the missing values and remove it from the data and see what version works better.


### IV. Model Building:


#### A. Decision Tree 

For our decision tree, we utilized functions from the tree and rpart library to fit our classification tree. In our baseline model based solely on the validation set, we found that only four variables (relationship, capital-gain, Edu_Mean_inc, and occupation) were used in the construction. There were 7 terminal nodes, and the misclassification rate was 16.03%.

To better fit this tree, we ran cross-validation to tune the tree complexity. With 10 folds, we found that the lowest cv error rate was 4060 cv errors, elbowing at 5 nodes. Deviation appeared to increase fairly linearly with k
```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics('~/images/decision_cv.png')
```

As such, we fit our classification tree by this size (pruning by different levels of k did not improve our accuracy rate), and found that our model to be the same as the baseline. 

To confirm these results, we also pruned through the rpart library, using the cp values. We found the cp of the smallest tree, that was within 1 standard deviation of the tree with the smallest xerror. Our best xerror was 0.64958 with xstd of 0.009372, so we selected the smallest tree with an error less than 0.658952. This is the 4th tree, with cp = 0.010.  Our test error is 0.006064632.

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics('~/images/decision_plotcp.png')
```

From this process, we again found that the pruned decision tree was equivalent to our baseline. The classification accuracy rate was 83.96806%. 

Our 5 most significant variables were relationship, Gen_Med_Mrg_Inc, marital_status, Edu_Mean_inc, and capital_gain. The graph shows the variable importance as a proportion for each feature.
```{r, echo=FALSE, fig.align='center'}
cartModel$variable.importance[1:5]
knitr::include_graphics('~/images/decision_varimp.png')
```


```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics('~/images/decision_tree.png')
```


We also examined t




Fit a classification tree (see examples in ISL chapter 8, and APM chapter 14).
# Make plots and describe the steps you took to justify choosing optimal tuning parameters.
# Report your 5 (or 6 or 7) important features (could be either just 5, or 6 or 7), with their variable importance statistics.
# Report the training accuracy rate.
# Plot the ROC curve, and report its area under the curve (AUC) statistic.


#### B. Bagged Trees 

For our

# The only parameters when bagging decision trees is the number of samples and
# hence the number of trees to include. This can be chosen by increasing the 
# number of trees on run after run until the accuracy begins to stop showing 
# improvement (e.g. on a cross validation test harness).

The top 7 most important features for this model, and their corresponding values, were relationship (100.000), fnlwgt (78.563), capital-gain (32.034), Edu_Mean_inc (31.506), hours-per-week (28.820), occupation (22.315), age(19.806), and occ_sex (14.616).

```{r, echo=FALSE}
knitr::include_graphics('~/images/Bagged_varImp_down_fit')
```

#### c. Random Forest

#### V. Results and Conclusion: